---
title: "Assignment 7"
author: "Erik Nylander"
date: "Saturday, March 14, 2015"
output: html_document
---

### Problem Set 1

#### Calculating Mean and Standard Deviation
For the first part of the problem I'll create a function that calculates the expected value(mean) and standard deviation. For comparisons sake I'm computing both the population and sample standard deviation so that the result of my function will also match the built in **sd()** function.
```{r}
MeanSTD <- function(v){
    # Calculates the mean and sample/poulation standard deviation of a given numerical vector.      
    #
    # Args:
    #   v = a numerical vector
    #
    # Returns:
    #   result = A list of the mean, sample, and population standard deviations. 
    
    mean <- sum(v)/length(v)
    sample.std <- sqrt(sum((v-mean)^2)/(length(v)-1)) #Sample standard deviation
    population.std <- sqrt(mean(v^2) - (mean(v))^2) #Population standard deviation
    result <- list("mean" = mean, "sampstd" = sample.std, "popstd" = population.std)
    return(result)
}
```

Now to test the above function and compare the results to the built in mean and standard deviation functions.

```{r}
set.seed(42)
test <- sample(100, 1000, replace=T) #Generating a random data set of numbers to test.
MeanSTD(test) 
mean(test)
sd(test)
```

As can bee seen from the results above the function computes an expected value and sample standard deviation that matches the values generated by the built in functions **mean()** and **sd()**. 

#### Stream of Numbers
For this problem I'll be calculating the mean by keeping track of the number of observations and the sum of the x's as we see them. I'll also use the sum of the x's along with the sum of the x's squared to compute the standard deviation using the formula $std = \frac{\sqrt{n \sum x^2 - (\sum x)^2}}{n}$. These values are stored in a csv file and the file is created in the working directory if is does not exist. The **EviroReset()** function is used to reset the csv file.
```{r}
EnviroReset <- function(){
    # Resets the values of n, sumx,sumxsq in the CurrentState.csv file
    #
    # Args:
    #   None
    # Returns:
    #   None
    
    n = 0
    sumx = 0
    sumxsq = 0
    init.state <- data.frame(n, sumx, sumxsq)
    write.csv(init.state, "CurrentState.csv")
}

RollingEstimate <- function(x){
    # Calculates the mean and standard deviation for a stream of values and stores the current state of these
    # calcualtions in the CurrenState.csv file.
    #
    # Args:
    #   x = The next value in the stream of data that will be used to update the mean and standard deviation
    #
    # Returns:
    #   result = A list that contains the current values for the mean and standard deviation
    
    if(!file.exists("CurrentState.csv")){
        EnviroReset() #Calls function to create the csv file if it does not exist
    }
    state <- read.csv("CurrentState.csv")
    n <- state$n
    sumx <- state$sumx
    sumxsq <- state$sumxsq
    xbar <- (sumx + x)/(n+1) # Calculates the current mean
    n <- n+1
    sumx <- sumx + x
    sumxsq <- sumxsq + x^2 
    sigma <- sqrt((n)*sumxsq - sumx^2)/n # Calculates the current standard deviation
    save.state <- data.frame(n, sumx, sumxsq)
    write.csv(save.state, "CurrentState.csv") # Saves out the updated values
    result <- list("mean" = xbar, "std" = sigma)
    return(result)
}
```

Now to test the functions. I've created a function, **RollTest()** that is used to to test the rolling estimates for the mean and standard deviation. The function generates random numbers and calls **RollingEstimate()** with this number. If reset is set to TRUE the CurrentState.csv file is reset to 0. The function prints out the state of the mean and standard deviation given the number of times it's called. For fun I also keep track of all of the estimates of the mean and plot these values to watch the mean converge.

```{r}
RollTest <- function(x, reset = FALSE){
    # Function for testing the RollingEstimate() function
    #
    # Args:
    #   x = number of iterations of the RollingEstimate() function to be called.
    #   reset = Do you want to reset the current environment? Defaults to FALSE
    #
    # Returns:
    #   nothing, but it does make an awesome plot!
    #
    
    library(ggplot2)
    if(reset == TRUE){
        EnviroReset()
    }
    vals <- c()
    xbar <- c()
    sigma <- c()
    for(i in 1:x){
        point <- 10*runif(1)
        estimate <- RollingEstimate(point)
        vals <- c(vals, i)
        xbar <- c(xbar, estimate$mean)
        sigma <- c(sigma, estimate$std)
    }
    data <- data.frame(vals, xbar, sigma)
    print(paste(("The rolling estimate of the mean is:"), estimate$mean))
    print(paste(("The rolling estimate of the standard deviation is:"), estimate$std))
    plot.mean <- ggplot(data, aes(x=vals, y=xbar)) + geom_line() + 
        xlab('Oservations') + ylab('Estimate of the Mean') +
        ggtitle('Rolling Estimate for the Mean')
    print(plot.mean)
    plot.sd <- ggplot(data, aes(x=vals, y=sigma)) + geom_line() + xlab('Oservations') + 
        ylab('Estimate of the Standard Deviation') +
        ggtitle('Rolling Estimate for the Standard Deviation')
    plot.sd
}

RollTest(1000, reset=TRUE)
```

This method of the calculating the mean and standard deviation does run into the issue that eventually the number of observations will get so large that there will be issues with R's ability to work with the value. The other option would be to use a rolling window of a given size and calculate the mean and standard deviation for all of the values in this window.

### Problem Set 2
#### Computing the Covariance and Correlation Matrices

First I'll read in the data from the auto-mpg file.
```{r}
auto.mpg <- read.table('auto-mpg.data', 
                       col.names=c('displacement', 'horsepower', 'weight', 'acceleration', 'mpg'))
```

Now to calculate the covariance and correlation matrices. First I'll create a function to take two vectors and calculate the covariance and correlation between the two.
```{r}
CovCor <- function(x, y){
    # Takes two vectors and calculates the covariance and correlation between the two vectors.
    #
    # Args:
    #   x = First vector of values.
    #   y = Second vector of values.
    #
    # Returns:
    #   result: A list containing the covariance, "cov", and the correlation, "cor"
    #
    
    mean.x <- mean(x)
    mean.y <- mean(y)
    sd.x <- sqrt(mean(x^2) - (mean(x))^2)
    sd.y <- sqrt(mean(y^2) - (mean(y))^2)
    covar <- mean((x - mean.x)*(y - mean.y))
    correl <- covar/(sd.x*sd.y)
    result <- list("cov" = covar, "cor" = correl)
    return(result)
}
```

Now to construct the correlation and covariance matrices and check them against the built in R functions.
```{r}
CovCorMatrix <- function(df){
    # Calculates the Covariance and Correlation matrices for a given data frame.
    #
    # Args:
    #   df = A dataframe of vectors used to calculate the covariance and correlation matrices.
    #
    # Returns:
    #   result = a list containing the covariance matrix "covmatrix" and correlation matrix "cormatrix"
    #
    
    cov.mat <- c()
    cor.mat <- c()
    for(i in 1:dim(df)[2]){
        cov.col <- c()
        cor.col <- c()
        for(j in 1:dim(df)[2]){
            temp <- CovCor(df[,i], df[,j])
            cov.col <- c(cov.col, temp$cov)
            cor.col <- c(cor.col, temp$cor)
        }
        cov.mat <- cbind(cov.mat, cov.col)
        cor.mat <- cbind(cor.mat, cor.col)
    }
    result <- list("covmatrix" = cov.mat, "cormatrix" = cor.mat)
    return(result)
}
```

Finally to check this function against the built in R functions.

```{r}
test <- CovCorMatrix(auto.mpg)

# Checking the Covariance Matrix
test$covmatrix
cov(auto.mpg)
```

These two matrices are similar with small difference in the values. I'm guessing that the difference may be involved in calculating of the standard deviation using sample vs. population standard deviations.

```{r}
# Checking the Correlation Matrix
test$cormatrix
cor(auto.mpg)
```

As we can see the two matrices are the same and have calculated the correlation matrix.

### Principal Component Analysis
From my understanding of the process we are looking to see which combination of the variables does the "best" job of explaining variance in the system of equations that is represented by the data in the matrix. This seems to be a similar process of projecting the matrix into the column space. For this problem I'll take a loot at what happens when we take out the **mpg** variable and see how the other values explain the variability of the **mpg**.
```{r}
auto.mpg.pca <- prcomp(auto.mpg[,-5], center = TRUE, scale = TRUE)
summary(auto.mpg.pca)
```
This gives us the importance of each of the principle components. We can look at the linear combination of the first principle component to see what variables have the largest impact. 
```{r}
auto.mpg.pca$rotation[,1]
```

From this we can see that Displacement, Horsepower, and Weight all seem to be negatively correlated with mpg which does make sense. Finally lets plot these findings.
```{r}
# Showing the variances explained by the principal componenets.
plot(auto.mpg.pca)

# Showing the biplot.
biplot(auto.mpg.pca, col=c('grey', 'red'))
```
This biplot also gives us a sense that the weight, displacement, and horsepower all have negative impacts on the mpg.